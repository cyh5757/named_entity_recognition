{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "\n",
    "# how to using huggingface tokenizer(bert) \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# using monologg/kobert tokenizer and model\n",
    "from kobert_transformers.tokenization_kobert import KoBertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "# huggingface Trainer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "#Cutomizer encoder\n",
    "from bert_dataset import TokenCollator\n",
    "from bert_dataset import TokenDataset\n",
    "\n",
    "\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO_tagging(tok_sentence,ne):\n",
    "  result=['O' for x in range(len(tok_sentence))]\n",
    "  ne_no=len(ne.keys())\n",
    "  if ne_no > 0:\n",
    "    for idx in range(1,ne_no+1):\n",
    "      ne_dict=ne[idx]\n",
    "      isbegin=True\n",
    "      for word_idx, word in enumerate(tok_sentence):\n",
    "        if '##' in word:\n",
    "          word=word.replace('##','')\n",
    "        if '▁' in word:\n",
    "          word=word.replace('_','')\n",
    "        if word in ne_dict['form']:\n",
    "          if isbegin:\n",
    "            result[word_idx]=str(ne_dict['label'][:2])+'_B'\n",
    "            isbegin=False\n",
    "            continue\n",
    "          elif isbegin==False & (('_B' in result[word_idx-1]) or '_I' in result[word_idx-1]):\n",
    "            result[word_idx]=str(ne_dict['label'][:2])+'_I'\n",
    "            continue\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels) -> None: #리스트의 리스트 형태의 데이터 입력\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "# 딕셔너리 값으로 리스트를 반환\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(texts, labels, valid_ratio=.2):\n",
    "\n",
    "    # define unique labels\n",
    "    unique_labels = ['O', 'PS_B', 'PS_I', 'LC_B', 'QT_B', 'QT_I', 'CV_B', 'CV_I', 'LC_I', 'OG_B', 'OG_I', 'DT_B', 'AM_B', 'DT_I', 'EV_B', 'EV_I', 'TM_B', 'TM_I', 'FD_B', 'FD_I', 'AM_I', 'AF_B', 'AF_I', 'TI_B', 'TI_I', 'PT_B', 'PT_I', 'TR_B', 'TR_I', 'MT_B', 'MT_I']\n",
    "\n",
    "    label_to_index = {}\n",
    "    index_to_label = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_index[label] = i\n",
    "        index_to_label[i] = label\n",
    "\n",
    "    # Convert label text to integer value\n",
    "    tags = []\n",
    "    for i in range(len(labels)):                                \n",
    "      tags.append(list(map(label_to_index.get, labels[i]))) \n",
    "\n",
    "    # Shuffle before split into train and validation set.\n",
    "    shuffled = list(zip(texts, tags))\n",
    "    random.shuffle(shuffled)\n",
    "    texts = [e[0] for e in shuffled]\n",
    "    labels = [e[1] for e in shuffled]\n",
    "    idx = int(len(texts) * (1 - valid_ratio))\n",
    "\n",
    "    train_dataset = TokenDataset(texts[:idx], labels[:idx])\n",
    "    valid_dataset = TokenDataset(texts[idx:], labels[idx:])\n",
    "\n",
    "    return train_dataset, valid_dataset, index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_input = tokenizer(example[\"text\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    previous_word_idx = None\n",
    "    labels = []\n",
    "    for word_idx in word_ids: \n",
    "      if word_idx is None:\n",
    "        labels.append(-100)\n",
    "      elif word_idx != previous_word_idx: \n",
    "        labels.append(example['label'][word_idx])\n",
    "      else:\n",
    "        labels.append(-100)\n",
    "      previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_input[\"labels\"] = labels\n",
    "    return tokenized_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/NER/data/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>늪 속으로 깊이 빠져드는 그 자신이다.</td>\n",
       "      <td>{}</td>\n",
       "      <td>N</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>절에 다니는 분들은 또 그거 가잖아</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>에~ 노무현 대통령이 하신 말씀 중에</td>\n",
       "      <td>{1: {'form': '노무현', 'label': 'PS_NAME', 'begin...</td>\n",
       "      <td>S</td>\n",
       "      <td>PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>뚜껑을 덮어 주시면</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미국 동부 명문 사립고인 쿠싱아카데미(Cushing Academy)의 짐 트레이시(...</td>\n",
       "      <td>{1: {'form': '미국', 'label': 'LCP_COUNTRY', 'be...</td>\n",
       "      <td>N</td>\n",
       "      <td>TM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624431</th>\n",
       "      <td>어쩌면 그거를 조금 더 빨리 발견할 수 있지 않나?</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624432</th>\n",
       "      <td>몰라</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624433</th>\n",
       "      <td>네.</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624434</th>\n",
       "      <td>음.</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624435</th>\n",
       "      <td>맞어 맞어.</td>\n",
       "      <td>{}</td>\n",
       "      <td>S</td>\n",
       "      <td>Out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624436 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "0                                   늪 속으로 깊이 빠져드는 그 자신이다.   \n",
       "1                                     절에 다니는 분들은 또 그거 가잖아   \n",
       "2                                    에~ 노무현 대통령이 하신 말씀 중에   \n",
       "3                                              뚜껑을 덮어 주시면   \n",
       "4       미국 동부 명문 사립고인 쿠싱아카데미(Cushing Academy)의 짐 트레이시(...   \n",
       "...                                                   ...   \n",
       "624431                       어쩌면 그거를 조금 더 빨리 발견할 수 있지 않나?   \n",
       "624432                                                 몰라   \n",
       "624433                                                 네.   \n",
       "624434                                                 음.   \n",
       "624435                                             맞어 맞어.   \n",
       "\n",
       "                                                    label source  \\\n",
       "0                                                      {}      N   \n",
       "1                                                      {}      S   \n",
       "2       {1: {'form': '노무현', 'label': 'PS_NAME', 'begin...      S   \n",
       "3                                                      {}      S   \n",
       "4       {1: {'form': '미국', 'label': 'LCP_COUNTRY', 'be...      N   \n",
       "...                                                   ...    ...   \n",
       "624431                                                 {}      S   \n",
       "624432                                                 {}      S   \n",
       "624433                                                 {}      S   \n",
       "624434                                                 {}      S   \n",
       "624435                                                 {}      S   \n",
       "\n",
       "       sentence_class  \n",
       "0                 Out  \n",
       "1                 Out  \n",
       "2                  PS  \n",
       "3                 Out  \n",
       "4                  TM  \n",
       "...               ...  \n",
       "624431            Out  \n",
       "624432            Out  \n",
       "624433            Out  \n",
       "624434            Out  \n",
       "624435            Out  \n",
       "\n",
       "[624436 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.18.0\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Suraj Patil, Stas Bekman, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.7/dist-packages\n",
      "Requires: filelock, pyyaml, numpy, tokenizers, huggingface-hub, packaging, sacremoses, tqdm, regex, requests, importlib-metadata\n",
      "Required-by: kobert-transformers\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=df['sentence']\n",
    "labels=df['label'].apply(lambda x:eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "model = AutoModelForTokenClassification.from_pretrained('klue/bert-base',num_labels=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['늪', '속', '##으로', '깊이', '빠져', '##드', '##는', '그', '자신', '##이다', '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tok=[]\n",
    "for sentence in sentences:\n",
    "  if len(sentence) < 512:\n",
    "    sentence_tok.append(tokenizer.tokenize(sentence))\n",
    "  else:\n",
    "    sentence_tok.append(tokenizer.tokenize(sentence,max_length=len(sentence),truncation=True))\n",
    "sentence_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_sentence=[]\n",
    "for tok_list,label in zip(sentence_tok,labels):\n",
    "  try:\n",
    "    bio_sentence+=[BIO_tagging(tok_list,label)]\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(tok_list)\n",
    "    print(label)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '동부', '명문', '사립', '##고', '##인', '쿠', '##싱', '##아', '##카', '##데미', '(', 'C', '##ush', '##ing', 'Ac', '##ade', '##my', ')', '의', '짐', '트레이', '##시', '(', 'Tr', '##acy', ')', '교장', '##이', '방한', '##했', '##다', '.']\n",
      "['LC_B', 'TM_B', 'O', 'O', 'O', 'O', 'OG_B', 'OG_I', 'OG_I', 'OG_I', 'OG_I', 'O', 'OG_B', 'OG_I', 'OG_I', 'OG_I', 'OG_I', 'OG_I', 'O', 'O', 'PS_B', 'PS_I', 'PS_I', 'O', 'PS_B', 'PS_I', 'O', 'CV_B', 'PS_I', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tok[4])\n",
    "print(bio_sentence[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert',num_labels=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'max_length': 536, 'truncation': True} not recognized.\n",
      "Keyword arguments {'max_length': 1326, 'truncation': True} not recognized.\n",
      "Keyword arguments {'max_length': 580, 'truncation': True} not recognized.\n",
      "Keyword arguments {'max_length': 638, 'truncation': True} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁', '늪', '▁속', '으로', '▁깊이', '▁빠져', '드는', '▁그', '▁자신이', '다', '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tok=[]\n",
    "for sentence in sentences:\n",
    "  if len(sentence) < 512:\n",
    "    sentence_tok.append(tokenizer.tokenize(sentence))\n",
    "  else:\n",
    "    sentence_tok.append(tokenizer.tokenize(sentence,max_length=len(sentence),truncation=True))\n",
    "sentence_tok[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_sentence=[]\n",
    "for tok_list,label in zip(sentence_tok,labels):\n",
    "  try:\n",
    "    bio_sentence+=[BIO_tagging(tok_list,label)]\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(tok_list)\n",
    "    print(label)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '동부', '명문', '사립', '##고', '##인', '쿠', '##싱', '##아', '##카', '##데미', '(', 'C', '##ush', '##ing', 'Ac', '##ade', '##my', ')', '의', '짐', '트레이', '##시', '(', 'Tr', '##acy', ')', '교장', '##이', '방한', '##했', '##다', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'OG_B', 'OG_I', 'O', 'OG_B', 'OG_I', 'OG_I', 'OG_I', 'O', 'PS_B', 'OG_I', 'OG_I', 'OG_I', 'PS_I', 'O', 'O', 'O', 'O', 'PS_B', 'PS_I', 'O', 'PS_I', 'PS_I', 'PS_I', 'PS_I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tok[4])\n",
    "print(bio_sentence[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentence_tok, bio_sentence, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, index_to_label = get_datasets(X_train, y_train, valid_ratio=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
